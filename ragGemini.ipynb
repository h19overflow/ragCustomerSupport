{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e00e130",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Adonis\\anaconda3\\envs\\RagGemini\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/bitext/bitext-gen-ai-chatbot-customer-support-dataset?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2.87M/2.87M [00:01<00:00, 2.05MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\Adonis\\.cache\\kagglehub\\datasets\\bitext\\bitext-gen-ai-chatbot-customer-support-dataset\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"bitext/bitext-gen-ai-chatbot-customer-support-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4f60d5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported and configuration set.\n",
      "GCP Project ID (placeholder): raggemini-459500\n",
      "Dataset Path: Bitext_Sample_Customer_Support_Training_Dataset_27K_responses-v11.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup - Importing Libraries and Configuration\n",
    "\n",
    "# --- Core Python Libraries ---\n",
    "import os  # For interacting with the operating system (e.g., checking file paths)\n",
    "import pandas as pd  # For data manipulation, especially working with CSV files (like our Bitext dataset)\n",
    "import numpy as np  # For numerical operations, particularly for handling embeddings as arrays\n",
    "import faiss  # A library from Facebook AI for efficient similarity search in vector collections\n",
    "from tqdm import tqdm  # A utility to show progress bars for loops, making long processes more user-friendly\n",
    "\n",
    "# --- Google Cloud Vertex AI SDK ---\n",
    "# This SDK allows us to interact with Google Cloud's AI services, including Gemini and Embedding models\n",
    "import vertexai\n",
    "from vertexai.language_models import TextEmbeddingModel # For generating text embeddings\n",
    "from vertexai.generative_models import GenerativeModel, Part # For using Gemini to generate text\n",
    "\n",
    "# --- Configuration ---\n",
    "# These are settings you'll need to adjust for your environment.\n",
    "\n",
    "# Replace with your Google Cloud Project ID. You can find this in your GCP console.\n",
    "GCP_PROJECT_ID = \"raggemini-459500\"\n",
    "\n",
    "# Replace with the Google Cloud region where you want to run Vertex AI services.\n",
    "# 'us-central1' is a common choice, but others are available.\n",
    "GCP_REGION = \"europe-central2\"\n",
    "\n",
    "# This is the path to the Bitext dataset CSV file you downloaded from Kaggle.\n",
    "# Make sure this file is in the same directory as your script, or provide the full path.\n",
    "DATASET_PATH = \"Bitext_Sample_Customer_Support_Training_Dataset_27K_responses-v11.csv\"\n",
    "\n",
    "# This specifies which embedding model we'll use from Vertex AI.\n",
    "# \"textembedding-gecko@003\" is one of Google's text embedding models.\n",
    "# Embeddings turn text into numerical vectors, capturing its meaning.\n",
    "EMBEDDING_MODEL_NAME = \"text-embedding-004\"\n",
    "\n",
    "# --- RAG Parameters ---\n",
    "# When we retrieve information, how many of the most relevant pieces (chunks) should we get?\n",
    "TOP_K_RETRIEVAL = 3 # We'll retrieve the top 3 most similar chunks for a given query.\n",
    "\n",
    "print(\"Libraries imported and configuration set.\")\n",
    "print(f\"GCP Project ID (placeholder): {GCP_PROJECT_ID}\")\n",
    "print(f\"Dataset Path: {DATASET_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e2e92a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertex AI initialized successfully for project 'raggemini-459500' in region 'europe-central2'.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Initialize Vertex AI\n",
    "\n",
    "# This step connects our script to your Google Cloud project and prepares Vertex AI for use.\n",
    "try:\n",
    "    vertexai.init(project=GCP_PROJECT_ID, location=GCP_REGION)\n",
    "    print(f\"Vertex AI initialized successfully for project '{GCP_PROJECT_ID}' in region '{GCP_REGION}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing Vertex AI: {e}\")\n",
    "    print(\"Please ensure:\")\n",
    "    print(f\"  1. You have replaced 'your-gcp-project-id' with your actual GCP Project ID in Cell 1.\")\n",
    "    print(f\"  2. The Vertex AI API is enabled in your GCP project: https://console.cloud.google.com/apis/library/aiplatform.googleapis.com\")\n",
    "    print(f\"  3. You have authenticated your environment (e.g., by running 'gcloud auth application-default login' in your terminal).\")\n",
    "    # exit() # Uncomment to stop execution if initialization fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bd62ad77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 1: Loading and Preprocessing Data from Bitext_Sample_Customer_Support_Training_Dataset_27K_responses-v11.csv ---\n",
      "Successfully loaded dataset. Found 26872 rows and 5 columns.\n",
      "Original number of rows: 26872\n",
      "Number of rows after dropping those with missing 'response' or 'instruction': 26872\n",
      "Text cleaning (stripping whitespace) applied.\n",
      "We will use 'response_cleaned' as our knowledge chunks.\n",
      "Columns in the DataFrame after preprocessing: ['flags', 'instruction', 'category', 'intent', 'response', 'response_cleaned', 'instruction_cleaned']\n",
      "\n",
      "Sample of the preprocessed data:\n",
      "                                 instruction_cleaned  \\\n",
      "0   question about cancelling order {{Order Number}}   \n",
      "1  i have a question about cancelling oorder {{Or...   \n",
      "2    i need help cancelling puchase {{Order Number}}   \n",
      "3         I need to cancel purchase {{Order Number}}   \n",
      "4  I cannot afford this order, cancel purchase {{...   \n",
      "\n",
      "                                    response_cleaned        intent category  \n",
      "0  I've understood you have a question regarding ...  cancel_order    ORDER  \n",
      "1  I've been informed that you have a question ab...  cancel_order    ORDER  \n",
      "2  I can sense that you're seeking assistance wit...  cancel_order    ORDER  \n",
      "3  I understood that you need assistance with can...  cancel_order    ORDER  \n",
      "4  I'm sensitive to the fact that you're facing f...  cancel_order    ORDER  \n",
      "\n",
      "Successfully loaded and preprocessed 26872 entries.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load and Preprocess the Bitext Dataset\n",
    "\n",
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads the Bitext dataset from a CSV file, performs basic cleaning,\n",
    "    and prepares it for use in our RAG system.\n",
    "\n",
    "    The Bitext dataset structure:\n",
    "    - \"instruction\": The user's query or question. (We'll use this for testing our RAG system).\n",
    "    - \"response\": The chatbot's answer. (This is the core of our knowledge base for RAG).\n",
    "    - \"intent\", \"category\": Additional metadata that can be useful for organizing or filtering.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Step 1: Loading and Preprocessing Data from {file_path} ---\")\n",
    "\n",
    "    # --- What is this dataset? ---\n",
    "    # The Bitext dataset contains pairs of user questions (\"instruction\") and chatbot answers (\"response\").\n",
    "    # This is perfect for RAG:\n",
    "    # - The \"response\" texts are the factual information we want our LLM to use. These will form our knowledge base.\n",
    "    # - The \"instruction\" texts can be used as example queries to test our RAG system.\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Successfully loaded dataset. Found {len(df)} rows and {len(df.columns)} columns.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Dataset file not found at '{file_path}'\")\n",
    "        print(\"Please ensure the DATASET_PATH in Cell 1 is correct and the file exists.\")\n",
    "        print(\"You can download it from: https://www.kaggle.com/datasets/bitext/bitext-gen-ai-chatbot-customer-support-dataset\")\n",
    "        return None # Return None if file not found\n",
    "\n",
    "    # --- Preprocessing ---\n",
    "    # Preprocessing involves cleaning and preparing the text data.\n",
    "\n",
    "    # 1. Handle missing values:\n",
    "    #    For RAG, both 'response' (our knowledge) and 'instruction' (for testing queries) are crucial.\n",
    "    #    If either is missing for a row, that row isn't very useful.\n",
    "    print(f\"Original number of rows: {len(df)}\")\n",
    "    df.dropna(subset=['response', 'instruction'], inplace=True)\n",
    "    print(f\"Number of rows after dropping those with missing 'response' or 'instruction': {len(df)}\")\n",
    "\n",
    "    # Reset row indices after dropping rows\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # 2. Clean the text: (Optional, dataset seems relatively clean)\n",
    "    #    Remove any irrelevant formatting or metadata if necessary.\n",
    "    #    Here, we'll just strip leading/trailing whitespace.\n",
    "    df['response_cleaned'] = df['response'].astype(str).str.strip()\n",
    "    df['instruction_cleaned'] = df['instruction'].astype(str).str.strip()\n",
    "\n",
    "    # --- Chunking Strategy ---\n",
    "    # \"Chunking\" means breaking down long texts into smaller, meaningful pieces.\n",
    "    # Why? Because embedding models work best on smaller pieces of text, and retrieval\n",
    "    # is more precise if chunks are focused.\n",
    "    #\n",
    "    # For this Bitext dataset, each \"response\" is already a Q&A pair's answer.\n",
    "    # These answers are often naturally good \"chunks\" because they typically address a specific point.\n",
    "    # If a single \"response\" was very long and covered many unrelated points, we might consider\n",
    "    # splitting it further (e.g., by sentences or paragraphs). For now, we'll treat each 'response' as one chunk.\n",
    "    print(\"Text cleaning (stripping whitespace) applied.\")\n",
    "    print(f\"We will use 'response_cleaned' as our knowledge chunks.\")\n",
    "\n",
    "    # We can also keep 'intent' and 'category' as metadata associated with each chunk.\n",
    "    # This could be useful for more advanced retrieval strategies later (e.g., filtering by category).\n",
    "    print(\"Columns in the DataFrame after preprocessing:\", df.columns.tolist())\n",
    "    print(\"\\nSample of the preprocessed data:\")\n",
    "    print(df[['instruction_cleaned', 'response_cleaned', 'intent', 'category']].head())\n",
    "\n",
    "    return df\n",
    "\n",
    "# --- Execute the function ---\n",
    "# Check if GCP_PROJECT_ID is set\n",
    "if GCP_PROJECT_ID == \"your-gcp-project-id\":\n",
    "    print(\"STOP: Please set your 'GCP_PROJECT_ID' in Cell 1 before proceeding.\")\n",
    "elif not os.path.exists(DATASET_PATH):\n",
    "    print(f\"STOP: Dataset file '{DATASET_PATH}' not found. Please check the path in Cell 1.\")\n",
    "else:\n",
    "    bitext_df = load_and_preprocess_data(DATASET_PATH)\n",
    "    if bitext_df is not None:\n",
    "        print(f\"\\nSuccessfully loaded and preprocessed {len(bitext_df)} entries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "74ed36a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2: Initializing AI Models ---\n",
      "Successfully loaded embedding model: 'text-embedding-004'\n",
      "Successfully loaded generative model: gemini-2.0-flash-lite-001\n",
      "\n",
      "Both embedding and generative models initialized.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Initialize AI Models (Embedding and Generative)\n",
    "from vertexai.preview.generative_models import GenerativeModel\n",
    "import vertexai\n",
    "\n",
    "vertexai.init(project=GCP_PROJECT_ID, location=GCP_REGION)\n",
    "def initialize_models():\n",
    "    \"\"\"\n",
    "    Initializes the text embedding model (to convert text to vectors)\n",
    "    and the generative LLM (to generate answers).\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Step 2: Initializing AI Models ---\")\n",
    "\n",
    "    # --- 1. Embedding Model ---\n",
    "    # What is an embedding model? It's a model that takes text as input and\n",
    "    # outputs a list of numbers (a \"vector\" or \"embedding\"). This vector\n",
    "    # numerically represents the meaning or semantic content of the text.\n",
    "    # Texts with similar meanings will have vectors that are \"close\" to each other\n",
    "    # in a high-dimensional space. This is key for finding relevant information.\n",
    "    try:\n",
    "        embedding_model = TextEmbeddingModel.from_pretrained(EMBEDDING_MODEL_NAME)\n",
    "        print(f\"Successfully loaded embedding model: '{EMBEDDING_MODEL_NAME}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading embedding model '{EMBEDDING_MODEL_NAME}': {e}\")\n",
    "        print(\"Ensure the model name is correct and you have permissions.\")\n",
    "        return None, None\n",
    "\n",
    "    # --- 2. Generative Model (LLM) ---\n",
    "    # This is the Large Language Model that will generate the final answer.\n",
    "    # We'll use one of Google's Gemini models.\n",
    "    # \"gemini-1.5-flash-001\" is chosen for a good balance of speed, cost, and capability.\n",
    "    # You could also use \"gemini-1.0-pro\" or \"gemini-1.5-pro-001\" (more capable, potentially slower/costlier).\n",
    "    try:\n",
    "        # generative_model = GenerativeModel(\"gemini-1.0-pro-001\")\n",
    "        generative_model = GenerativeModel(\"gemini-2.0-flash-lite-001\")\n",
    "        print(f\"Successfully loaded generative model: gemini-2.0-flash-lite-001\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading generative model: {e}\")\n",
    "        print(\"Ensure the model name is correct and you have permissions.\")\n",
    "        return embedding_model, None # Return embedding model if it loaded\n",
    "\n",
    "    return embedding_model, generative_model\n",
    "\n",
    "# --- Execute the function ---\n",
    "if 'bitext_df' in locals() and bitext_df is not None: # Check if previous step was successful\n",
    "    embedding_model, generative_model = initialize_models()\n",
    "    if embedding_model and generative_model:\n",
    "        print(\"\\nBoth embedding and generative models initialized.\")\n",
    "    else:\n",
    "        print(\"\\nModel initialization failed. Please check error messages above.\")\n",
    "else:\n",
    "    print(\"Skipping model initialization because data loading failed or was not run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c8016b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=GCP_PROJECT_ID, location=GCP_REGION)\n",
    "\n",
    "# List all models available in the region\n",
    "models = aiplatform.Model.list()\n",
    "for model in models:\n",
    "    print(f\"Model ID: {model.display_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "934d8132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using a sample of 2000 entries from the dataset for faster demonstration.\n",
      "\n",
      "--- Step 3: Creating Embeddings and FAISS Vector Index ---\n",
      "Preparing to generate embeddings for 2000 response chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings: 100%|██████████| 40/40 [00:59<00:00,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully generated 2000 embeddings, each with dimension 768.\n",
      "FAISS index created and populated with 2000 vectors.\n",
      "The knowledge base is now indexed and ready for searching.\n",
      "\n",
      "FAISS index built successfully. Contains 2000 items.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Create Embeddings and Build the Vector Index (Knowledge Base)\n",
    "\n",
    "# --- RAG Core Concept: Indexing the Knowledge Base ---\n",
    "# To quickly find relevant information, we need to:\n",
    "# 1. Convert all our knowledge \"chunks\" (the 'response_cleaned' texts) into embeddings (numerical vectors).\n",
    "# 2. Store these embeddings in a special database called a \"Vector Index\" or \"Vector Database\".\n",
    "#    This database is optimized for finding vectors that are \"similar\" to a query vector.\n",
    "#\n",
    "# We are using FAISS for this example. FAISS is a library for efficient similarity search.\n",
    "# Other options include Pinecone, Weaviate, Chroma, or Vertex AI Vector Search (for cloud-native).\n",
    "\n",
    "def create_embeddings_and_index(df_to_index, emb_model):\n",
    "    \"\"\"\n",
    "    Generates embeddings for all 'response_cleaned' texts (our knowledge chunks)\n",
    "    and stores them in a FAISS vector index.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Step 3: Creating Embeddings and FAISS Vector Index ---\")\n",
    "    if df_to_index is None or df_to_index.empty:\n",
    "        print(\"DataFrame is empty or None. Cannot create embeddings.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    # These lists will store the original texts and their metadata,\n",
    "    # corresponding to the order of embeddings in the FAISS index.\n",
    "    # This allows us to retrieve the actual text once we find a similar embedding.\n",
    "    knowledge_base_texts = []\n",
    "    knowledge_base_metadata = [] # e.g., intent, category, original instruction\n",
    "\n",
    "    # Get the 'response_cleaned' column, which contains our knowledge chunks.\n",
    "    responses_to_embed = df_to_index['response_cleaned'].tolist()\n",
    "    print(f\"Preparing to generate embeddings for {len(responses_to_embed)} response chunks...\")\n",
    "\n",
    "    all_embeddings_list = []\n",
    "    # Process in batches to be efficient and avoid hitting API limits if any.\n",
    "    # The get_embeddings method in Vertex AI SDK can handle batches.\n",
    "    # textembedding-gecko models typically have a batch limit (e.g., 250 for @001, check docs for @003)\n",
    "    batch_size = 50 # You can adjust this based on model limits and performance\n",
    "    \n",
    "    for i in tqdm(range(0, len(responses_to_embed), batch_size), desc=\"Generating Embeddings\"):\n",
    "        batch_texts = responses_to_embed[i:i+batch_size]\n",
    "        try:\n",
    "            # The get_embeddings() method returns a list of TextEmbedding objects.\n",
    "            # Each object has a .values attribute containing the numerical vector.\n",
    "            embeddings_result = emb_model.get_embeddings(batch_texts)\n",
    "            batch_embeddings_vectors = [emb.values for emb in embeddings_result]\n",
    "            all_embeddings_list.extend(batch_embeddings_vectors)\n",
    "\n",
    "            # Store corresponding texts and metadata for this batch\n",
    "            knowledge_base_texts.extend(batch_texts)\n",
    "            for j in range(len(batch_texts)):\n",
    "                original_df_index = df_to_index.index[i + j] # Get original index from df_to_index\n",
    "                knowledge_base_metadata.append({\n",
    "                    'intent': df_to_index.loc[original_df_index, 'intent'],\n",
    "                    'category': df_to_index.loc[original_df_index, 'category'],\n",
    "                    'original_instruction': df_to_index.loc[original_df_index, 'instruction_cleaned']\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error embedding batch starting at index {i}: {e}\")\n",
    "            # For simplicity, we'll skip failed batches, but in a real app, you might retry or log.\n",
    "            # If errors are frequent, check text lengths, API quotas, or special characters.\n",
    "            continue # Skip to the next batch\n",
    "\n",
    "    if not all_embeddings_list:\n",
    "        print(\"No embeddings were generated. Cannot create FAISS index.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    # Convert the list of embedding vectors into a NumPy array of type float32, required by FAISS.\n",
    "    embeddings_np = np.array(all_embeddings_list).astype('float32')\n",
    "    \n",
    "    # The dimension of the embeddings (e.g., 768 for textembedding-gecko).\n",
    "    embedding_dimension = embeddings_np.shape[1]\n",
    "    print(f\"\\nSuccessfully generated {len(embeddings_np)} embeddings, each with dimension {embedding_dimension}.\")\n",
    "\n",
    "    # --- Create FAISS Index ---\n",
    "    # faiss.IndexFlatL2: This creates a simple index that performs an exhaustive search\n",
    "    # using L2 distance (Euclidean distance). For normalized embeddings (like those from\n",
    "    # textembedding-gecko), L2 distance is equivalent to maximizing cosine similarity.\n",
    "    # \"Flat\" means it doesn't use any complex structures to speed up search at the cost of accuracy.\n",
    "    # For very large datasets, more advanced FAISS indexes (e.g., IndexIVFFlat) might be used.\n",
    "    index = faiss.IndexFlatL2(embedding_dimension)\n",
    "    index.add(embeddings_np) # Add all our generated embeddings to the FAISS index.\n",
    "\n",
    "    print(f\"FAISS index created and populated with {index.ntotal} vectors.\")\n",
    "    print(\"The knowledge base is now indexed and ready for searching.\")\n",
    "\n",
    "    return index, knowledge_base_texts, knowledge_base_metadata, embedding_dimension\n",
    "\n",
    "# --- Execute the function ---\n",
    "# For demonstration, let's use a smaller subset of the data to speed up the embedding process.\n",
    "# In a real application, you'd likely use your full dataset.\n",
    "if 'bitext_df' in locals() and bitext_df is not None and \\\n",
    "   'embedding_model' in locals() and embedding_model is not None:\n",
    "\n",
    "    sample_size = 2000 # Number of responses to use for this demo. Adjust as needed.\n",
    "                       # Set to len(bitext_df) to use the whole dataset (will take longer).\n",
    "    if len(bitext_df) > sample_size:\n",
    "        print(f\"\\nUsing a sample of {sample_size} entries from the dataset for faster demonstration.\")\n",
    "        # Make sure to use .copy() to avoid SettingWithCopyWarning if you modify df_sample later\n",
    "        bitext_df_sample = bitext_df.sample(n=sample_size, random_state=42).reset_index(drop=True).copy()\n",
    "    else:\n",
    "        print(f\"\\nUsing the full dataset of {len(bitext_df)} entries.\")\n",
    "        bitext_df_sample = bitext_df.copy()\n",
    "\n",
    "    faiss_index, kb_texts, kb_metadata, kb_embedding_dim = create_embeddings_and_index(\n",
    "        bitext_df_sample,\n",
    "        embedding_model\n",
    "    )\n",
    "    if faiss_index:\n",
    "        print(f\"\\nFAISS index built successfully. Contains {faiss_index.ntotal} items.\")\n",
    "else:\n",
    "    print(\"Skipping embedding creation because previous steps (data loading or model init) failed or were not run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c4f5dc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing Retrieval for query: 'How do I reset my password?' ---\n",
      "Retrieved 3 chunks for the test query:\n",
      "  Chunk 1 (Distance: 0.4772, Sim Score: 0.7614):\n",
      "    Text: I can see that you're unsure about how to reset the password for your user account. Don't worry, I'm here to assist you every step of the way! To init...\n",
      "    Intent: recover_password\n",
      "  Chunk 2 (Distance: 0.4820, Sim Score: 0.7590):\n",
      "    Text: Assuredly! I'm here to assist you in resetting your account password. It's essential to keep your account secure and ensure that only you have access ...\n",
      "    Intent: recover_password\n",
      "  Chunk 3 (Distance: 0.4896, Sim Score: 0.7552):\n",
      "    Text: For sure! I completely understand your situation and the importance of regaining access to your user account. Allow me to guide you through the proces...\n",
      "    Intent: recover_password\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Implement the Retrieval Mechanism\n",
    "\n",
    "# --- RAG Core Concept: Retrieval ---\n",
    "# Now that our knowledge base is indexed, we can implement the \"Retrieval\" step.\n",
    "# When a user asks a question (query):\n",
    "# 1. We convert the user's query into an embedding (using the SAME embedding model we used for the knowledge base).\n",
    "# 2. We use the FAISS index to search for the 'k' embeddings in our knowledge base\n",
    "#    that are most similar (closest) to the query embedding.\n",
    "# 3. We then retrieve the original text chunks corresponding to these similar embeddings.\n",
    "#    These are the pieces of information most likely to help answer the user's query.\n",
    "\n",
    "def retrieve_relevant_chunks(query_text, emb_model, vector_index,\n",
    "                             original_texts, original_metadata,\n",
    "                             num_chunks_to_retrieve):\n",
    "    \"\"\"\n",
    "    1. Embeds the user's query.\n",
    "    2. Searches the FAISS index for the most similar chunks from the knowledge base.\n",
    "    3. Returns the text and metadata of these retrieved chunks.\n",
    "    \"\"\"\n",
    "    # print(f\"\\n--- Step 4a: Retrieving relevant chunks for query: '{query_text[:100]}...' ---\")\n",
    "\n",
    "    if vector_index is None or not original_texts:\n",
    "        print(\"Vector index or original texts not available. Cannot retrieve.\")\n",
    "        return []\n",
    "\n",
    "    # 1. Generate embedding for the user's query.\n",
    "    #    It's CRUCIAL to use the same embedding model that was used to create the knowledge base.\n",
    "    try:\n",
    "        query_embedding_obj = emb_model.get_embeddings([query_text]) # Must be a list\n",
    "        query_vector = np.array(query_embedding_obj[0].values).astype('float32').reshape(1, -1)\n",
    "        # .reshape(1, -1) ensures it's a 2D array, as FAISS expects a batch of query vectors.\n",
    "    except Exception as e:\n",
    "        print(f\"Error embedding query '{query_text}': {e}\")\n",
    "        return []\n",
    "\n",
    "    # 2. Perform similarity search in the FAISS index.\n",
    "    #    `vector_index.search()` returns:\n",
    "    #    - `distances`: The L2 distances of the found chunks to the query vector. Smaller is better.\n",
    "    #    - `indices`: The indices (positions) of these chunks in the original `embeddings_np`\n",
    "    #                 (and thus in our `original_texts` and `original_metadata` lists).\n",
    "    # print(f\"Searching for {num_chunks_to_retrieve} closest chunks in the index of {vector_index.ntotal} items.\")\n",
    "    distances, indices = vector_index.search(query_vector, num_chunks_to_retrieve)\n",
    "\n",
    "    retrieved_chunks_info = []\n",
    "    # print(f\"Found {len(indices[0])} potential matches.\")\n",
    "    for i in range(len(indices[0])):\n",
    "        idx = indices[0][i] # Get the index of the i-th retrieved chunk\n",
    "        \n",
    "        # FAISS can return -1 if it can't find enough valid neighbors (e.g. k > ntotal)\n",
    "        if idx == -1: \n",
    "            # print(f\"Warning: FAISS returned -1 for index {i}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        if idx >= len(original_texts):\n",
    "            # print(f\"Warning: Retrieved index {idx} is out of bounds for original_texts (len: {len(original_texts)}). Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        dist = distances[0][i]\n",
    "        \n",
    "        # For L2 distance on normalized vectors, similarity can be represented as (e.g.) 1 - dist or 2 - dist^2.\n",
    "        # A simple pseudo-similarity: higher is better.\n",
    "        # Cosine similarity = (2 - D^2) / 2 for normalized vectors where D is Euclidean distance.\n",
    "        # Or simply, smaller distance means more similar.\n",
    "        # We'll store the distance itself, or a transformed score. Let's use 1 - distance as a rough similarity.\n",
    "        # (Note: This isn't a true probability, just a score for ranking.)\n",
    "        similarity_score = 1 - (dist / 2) # A rough score, max 1 if dist is 0. Max L2 dist for normalized vectors is 2.\n",
    "\n",
    "        retrieved_chunks_info.append({\n",
    "            'text': original_texts[idx],\n",
    "            'metadata': original_metadata[idx],\n",
    "            'distance': float(dist), # Store the actual distance\n",
    "            'similarity_score': float(similarity_score) # Approximate similarity\n",
    "        })\n",
    "        # print(f\"  Retrieved Chunk {i+1} (Index: {idx}, Distance: {dist:.4f}): {original_texts[idx][:100]}...\")\n",
    "\n",
    "    # Sort by distance (ascending) or similarity_score (descending) just to be sure,\n",
    "    # though FAISS usually returns them sorted by distance.\n",
    "    retrieved_chunks_info.sort(key=lambda x: x['distance'])\n",
    "    \n",
    "    return retrieved_chunks_info\n",
    "\n",
    "# --- Example of using the retrieval function (will be used more in the next cell) ---\n",
    "if 'faiss_index' in locals() and faiss_index is not None:\n",
    "    test_query_for_retrieval = \"How do I reset my password?\"\n",
    "    print(f\"\\n--- Testing Retrieval for query: '{test_query_for_retrieval}' ---\")\n",
    "    retrieved_for_test = retrieve_relevant_chunks(\n",
    "        test_query_for_retrieval,\n",
    "        embedding_model,\n",
    "        faiss_index,\n",
    "        kb_texts, # from Cell 5\n",
    "        kb_metadata, # from Cell 5\n",
    "        TOP_K_RETRIEVAL # from Cell 1\n",
    "    )\n",
    "    if retrieved_for_test:\n",
    "        print(f\"Retrieved {len(retrieved_for_test)} chunks for the test query:\")\n",
    "        for i, chunk_info in enumerate(retrieved_for_test):\n",
    "            print(f\"  Chunk {i+1} (Distance: {chunk_info['distance']:.4f}, Sim Score: {chunk_info['similarity_score']:.4f}):\")\n",
    "            print(f\"    Text: {chunk_info['text'][:150]}...\")\n",
    "            print(f\"    Intent: {chunk_info['metadata']['intent']}\")\n",
    "    else:\n",
    "        print(\"No chunks retrieved for the test query, or an error occurred.\")\n",
    "else:\n",
    "    print(\"Skipping retrieval test because FAISS index is not available from previous steps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f810fd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing Generation for query: 'How do I reset my password?' with 3 retrieved chunks ---\n",
      "\n",
      "--- RAG System Generated Response (Test) ---\n",
      "To reset your password:\n",
      "\n",
      "1.  Go to the login page.\n",
      "2.  Click on the \"Forgot Password\" option.\n",
      "3.  Enter the email address associated with your account.\n",
      "4.  Check your inbox (and spam/junk folders) for an email with instructions.\n",
      "5.  Follow the instructions in the email to create a new password.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Integrate with Gemini for Generation (Augmentation & Generation)\n",
    "\n",
    "# --- RAG Core Concept: Augmentation & Generation ---\n",
    "# This is where the \"Augmented Generation\" part of RAG happens.\n",
    "# 1. Augmentation: We take the original user query AND the relevant chunks we just retrieved.\n",
    "#    We combine them into a single, detailed \"prompt\" for the LLM (Gemini).\n",
    "#    This prompt essentially tells Gemini: \"Here's the user's question, and here's some specific\n",
    "#    information from our knowledge base that should help you answer it. Please use this information.\"\n",
    "#\n",
    "# 2. Generation: We send this \"augmented prompt\" to the LLM (Gemini).\n",
    "#    The LLM then generates an answer, hopefully more accurate and context-aware\n",
    "#    because it has been \"grounded\" in the retrieved factual information.\n",
    "\n",
    "def generate_response_with_gemini(user_query, retrieved_chunks_list, gen_model):\n",
    "    \"\"\"\n",
    "    Constructs a prompt for Gemini including the user query and retrieved context,\n",
    "    then calls the Gemini API to generate a response.\n",
    "    \"\"\"\n",
    "    # print(\"\\n--- Step 4b: Generating response with Gemini using retrieved context ---\")\n",
    "\n",
    "    # 1. Construct the context string from the retrieved chunks.\n",
    "    #    We'll present each chunk clearly to the LLM.\n",
    "    context_str_parts = []\n",
    "    if retrieved_chunks_list:\n",
    "        for i, chunk_info in enumerate(retrieved_chunks_list):\n",
    "            context_str_parts.append(f\"Context Chunk {i+1} (Source: Customer Support Response):\\n{chunk_info['text']}\")\n",
    "        context_for_prompt = \"\\n\\n\".join(context_str_parts)\n",
    "    else:\n",
    "        context_for_prompt = \"No relevant context was found.\"\n",
    "        # print(\"Warning: No context chunks provided to Gemini. LLM will answer from its general knowledge.\")\n",
    "\n",
    "    # 2. Construct the full prompt for Gemini.\n",
    "    #    This is a critical step called \"Prompt Engineering\". The way you phrase the prompt\n",
    "    #    can significantly affect the quality of the LLM's answer.\n",
    "    #    We instruct the LLM on its role, how to use the context, and what to do if\n",
    "    #    the context isn't sufficient.\n",
    "    prompt = f\"\"\"You are a helpful and concise customer support assistant.\n",
    "Your goal is to answer the user's query based *only* on the provided context.\n",
    "\n",
    "Here is the user's query:\n",
    "\"{user_query}\"\n",
    "\n",
    "Here is the context retrieved from our knowledge base:\n",
    "--- BEGIN CONTEXT ---\n",
    "{context_for_prompt}\n",
    "--- END CONTEXT ---\n",
    "\n",
    "Please answer the user's query using *only* the information available in the 'BEGIN CONTEXT' and 'END CONTEXT' sections.\n",
    "If the provided context does not contain the information needed to answer the query, please state:\n",
    "\"I'm sorry, but the provided information from our knowledge base does not seem to contain a direct answer to your question.\"\n",
    "Do not make up information or use external knowledge. Be factual and stick to the provided text.\n",
    "If the context is relevant, synthesize the information to provide a clear and direct answer to the user's query.\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "    # print(\"\\n--- Augmented Prompt for Gemini (first 500 chars) ---\")\n",
    "    # print(prompt[:500] + \"...\") # For debugging, show the start of the prompt\n",
    "\n",
    "    # 3. Send the prompt to the Gemini API and get the response.\n",
    "    try:\n",
    "        # For gemini-1.5-flash or pro, we can send just the string prompt\n",
    "        response = gen_model.generate_content(prompt)\n",
    "        # print(\"\\n--- Gemini Raw Response Object ---\")\n",
    "        # print(response) # To see the full response object if needed\n",
    "\n",
    "        generated_text = response.text\n",
    "        # print(\"\\n--- Gemini Generated Text ---\")\n",
    "        # print(generated_text)\n",
    "        return generated_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response with Gemini: {e}\")\n",
    "        # More detailed error information if available from the API response\n",
    "        if hasattr(e, 'response') and e.response:\n",
    "            print(f\"API Error Details: {e.response}\")\n",
    "        elif hasattr(e, 'message'):\n",
    "            print(f\"Error Message: {e.message}\")\n",
    "        return \"I apologize, but I encountered an error while trying to generate a response.\"\n",
    "\n",
    "# --- Example of using the generation function (will be used more in the next cell) ---\n",
    "if 'generative_model' in locals() and generative_model is not None and 'retrieved_for_test' in locals():\n",
    "    if retrieved_for_test: # If we successfully retrieved chunks in the previous cell's test\n",
    "        print(f\"\\n--- Testing Generation for query: '{test_query_for_retrieval}' with {len(retrieved_for_test)} retrieved chunks ---\")\n",
    "        rag_test_response = generate_response_with_gemini(\n",
    "            test_query_for_retrieval,\n",
    "            retrieved_for_test,\n",
    "            generative_model\n",
    "        )\n",
    "        print(\"\\n--- RAG System Generated Response (Test) ---\")\n",
    "        print(rag_test_response)\n",
    "    else:\n",
    "        print(\"\\nSkipping generation test as no chunks were retrieved in the previous test.\")\n",
    "else:\n",
    "    print(\"Skipping generation test because generative model or retrieved chunks are not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bfc49610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- TESTING THE FULL RAG PIPELINE ---\n",
      "\n",
      "\n",
      "--- Test Case: Query from Dataset ---\n",
      "Original Instruction (User Query): list your accepted payment options\n",
      "Original Response (Expected): I'll make it happen! I'm here to provide you with a rundown of our accepted payment options. Here they are:\n",
      "\n",
      "- **Credit/Debit Card:** We accept major card brands such as Visa, Mastercard, and American Express.\n",
      "- **PayPal:** A secure and widely-used o...\n",
      "\n",
      "==================================================\n",
      "Processing Query: \"list your accepted payment options\"\n",
      "==================================================\n",
      "Step A: Retrieving relevant chunks...\n",
      "\n",
      "Retrieved 3 chunks for the query.\n",
      "  Retrieved Chunk 1 (Sim Score: 0.8719): Definitely! I'm here to provide you with the necessary guidance to view our accepted payment options...\n",
      "  Retrieved Chunk 2 (Sim Score: 0.8562): Certainly! I'm here to assist you with listing our accepted payment methods. To view the available o...\n",
      "  Retrieved Chunk 3 (Sim Score: 0.8542): No problem at all! I'll gladly assist you with finding the list of our accepted payment methods. To ...\n",
      "\n",
      "Step B: Generating response with Gemini...\n",
      "\n",
      "--- Final RAG System Response ---\n",
      "To view the accepted payment options, visit our website and navigate to the payment section or checkout page. Look for the \"Payment Methods\" or \"Accepted Payment Options\" tab to see the list. The available options include credit/debit cards, PayPal, bank transfers, Apple Pay, and Google Wallet.\n",
      "\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "--- Test Case: Query from Dataset ---\n",
      "Original Instruction (User Query): how do I update the shipping address?\n",
      "Original Response (Expected): Sure, I can help you with updating your shipping address. To make changes to your shipping address, you can log in to your account on our website and navigate to the \"My Account\" section. From there, you should be able to find an option to edit your ...\n",
      "\n",
      "==================================================\n",
      "Processing Query: \"how do I update the shipping address?\"\n",
      "==================================================\n",
      "Step A: Retrieving relevant chunks...\n",
      "\n",
      "Retrieved 3 chunks for the query.\n",
      "  Retrieved Chunk 1 (Sim Score: 0.8746): Certainly! If you need to update your shipping address, you can easily do so by following these step...\n",
      "  Retrieved Chunk 2 (Sim Score: 0.8736): Sure, I can help you with updating your shipping address. To make changes to your shipping address, ...\n",
      "  Retrieved Chunk 3 (Sim Score: 0.8725): Sure! I'm here to help you update your shipping address. Please provide me with the details of the n...\n",
      "\n",
      "Step B: Generating response with Gemini...\n",
      "\n",
      "--- Final RAG System Response ---\n",
      "To update your shipping address, log into your account on the website. Navigate to the \"My Account\" section and find the option to edit your shipping address. You can then edit your current address and enter your new shipping address, remembering to save the updated information. If you need further assistance, contact customer service.\n",
      "\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "--- Test Case: Query from Dataset ---\n",
      "Original Instruction (User Query): help me use the freemium profile\n",
      "Original Response (Expected): I'll do my best! I'm thrilled that you're interested in utilizing our {{Account Category}} profile. Let me assist you in navigating the features and maximizing your experience:\n",
      "\n",
      "1. **Sign in**: Begin by logging into our platform using your credential...\n",
      "\n",
      "==================================================\n",
      "Processing Query: \"help me use the freemium profile\"\n",
      "==================================================\n",
      "Step A: Retrieving relevant chunks...\n",
      "\n",
      "Retrieved 3 chunks for the query.\n",
      "  Retrieved Chunk 1 (Sim Score: 0.7715): I'm happy to help! I'm delighted to assist you in making the most of our Free profile:\n",
      "\n",
      "1. Sign In: ...\n",
      "  Retrieved Chunk 2 (Sim Score: 0.7687): Definitely! I appreciate your interest in utilizing our Free profile. Allow me to guide you through ...\n",
      "  Retrieved Chunk 3 (Sim Score: 0.7519): Thank you for reaching out with your inquiry about creating a freemium account. I'm here to provide ...\n",
      "\n",
      "Step B: Generating response with Gemini...\n",
      "\n",
      "--- Final RAG System Response ---\n",
      "Here's how to use the Free profile, according to the provided information:\n",
      "\n",
      "1.  **Sign In/Access Your Account:** Log in to the platform.\n",
      "2.  **Account Navigation:** Go to '{{Settings}}' or '{{Profile}}' section.\n",
      "3.  **Profile Management:** Customize your profile and access features.\n",
      "4.  **Select Free Profile:** Choose the '{{Profile Type}}' option.\n",
      "5.  **Final Steps:** Follow on-screen instructions.\n",
      "\n",
      "Enjoy the benefits of the Free profile, which include accessing basic features and limited content.\n",
      "\n",
      "---------------------------------\n",
      "\n",
      "--- RAG Pipeline Test Complete ---\n",
      "\n",
      "Considerations for Improvement & Iteration:\n",
      "1. Chunking Strategy: If responses are very long, split them into smaller, more focused chunks.\n",
      "2. TOP_K_RETRIEVAL: Experiment with the number of chunks retrieved (k).\n",
      "3. Prompt Engineering: Refine the prompt sent to Gemini for better control over output.\n",
      "4. Evaluation: Systematically evaluate responses (e.g., against 'expected_response' or with human review).\n",
      "5. Vector Database: For larger scale, consider Vertex AI Vector Search or other managed vector DBs.\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Run the Full RAG Pipeline and Test\n",
    "\n",
    "# Now we'll put all the pieces together to create a complete RAG pipeline\n",
    "# and test it with a few sample queries from our Bitext dataset.\n",
    "\n",
    "def run_rag_query(query_text, emb_model, gen_model, vec_index,\n",
    "                  knowledge_texts, knowledge_meta, top_k):\n",
    "    \"\"\"\n",
    "    Executes the full RAG pipeline for a single query:\n",
    "    1. Retrieves relevant chunks.\n",
    "    2. Generates a response using Gemini with the retrieved context.\n",
    "    \"\"\"\n",
    "    print(f\"\\n==================================================\")\n",
    "    print(f\"Processing Query: \\\"{query_text}\\\"\")\n",
    "    print(f\"==================================================\")\n",
    "\n",
    "    # 1. Retrieve relevant chunks\n",
    "    print(\"Step A: Retrieving relevant chunks...\")\n",
    "    retrieved_chunks = retrieve_relevant_chunks(\n",
    "        query_text,\n",
    "        emb_model,\n",
    "        vec_index,\n",
    "        knowledge_texts,\n",
    "        knowledge_meta,\n",
    "        top_k\n",
    "    )\n",
    "\n",
    "    if not retrieved_chunks:\n",
    "        print(\"No relevant chunks found for this query by the retrieval system.\")\n",
    "        # Optionally, you could still send to Gemini without context, or return a specific message.\n",
    "        # For this demo, we'll let Gemini try without explicit context if retrieval fails.\n",
    "        # However, the prompt in `generate_response_with_gemini` handles \"No relevant context was found.\"\n",
    "        # return \"Could not find relevant information in the knowledge base to answer this query.\"\n",
    "\n",
    "    print(f\"\\nRetrieved {len(retrieved_chunks)} chunks for the query.\")\n",
    "    for i, chunk_info in enumerate(retrieved_chunks):\n",
    "        print(f\"  Retrieved Chunk {i+1} (Sim Score: {chunk_info['similarity_score']:.4f}): {chunk_info['text'][:100]}...\")\n",
    "        # print(f\"    Intent: {chunk_info['metadata']['intent']}, Category: {chunk_info['metadata']['category']}\")\n",
    "\n",
    "\n",
    "    # 2. Generate response with Gemini (Augmentation & Generation)\n",
    "    print(\"\\nStep B: Generating response with Gemini...\")\n",
    "    final_response = generate_response_with_gemini(\n",
    "        query_text,\n",
    "        retrieved_chunks,\n",
    "        gen_model\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Final RAG System Response ---\")\n",
    "    print(final_response)\n",
    "    print(\"---------------------------------\")\n",
    "    return final_response\n",
    "\n",
    "# --- Main Test Execution ---\n",
    "if 'bitext_df' in locals() and bitext_df is not None and \\\n",
    "   'embedding_model' in locals() and embedding_model is not None and \\\n",
    "   'generative_model' in locals() and generative_model is not None and \\\n",
    "   'faiss_index' in locals() and faiss_index is not None and \\\n",
    "   'kb_texts' in locals() and kb_texts is not None and \\\n",
    "   'kb_metadata' in locals() and kb_metadata is not None:\n",
    "\n",
    "    print(\"\\n\\n--- TESTING THE FULL RAG PIPELINE ---\")\n",
    "\n",
    "    # Select a few \"instructions\" from our original Bitext dataset to use as test queries.\n",
    "    # We'll use instructions that were part of the sampled data used for indexing for a fair test of retrieval.\n",
    "    # If bitext_df_sample was used for indexing:\n",
    "    if 'bitext_df_sample' in locals() and not bitext_df_sample.empty:\n",
    "        num_test_queries = 3 # How many test queries to run\n",
    "        if len(bitext_df_sample) >= num_test_queries:\n",
    "            test_queries_df = bitext_df_sample.sample(n=num_test_queries, random_state=101)\n",
    "        else:\n",
    "            print(f\"Warning: Sampled data has less than {num_test_queries} entries. Using all available.\")\n",
    "            test_queries_df = bitext_df_sample\n",
    "    else: # Fallback to original df if sample not available\n",
    "        print(\"Warning: `bitext_df_sample` not found. Using original `bitext_df` for test queries.\")\n",
    "        test_queries_df = bitext_df.sample(n=min(3, len(bitext_df)), random_state=101)\n",
    "\n",
    "\n",
    "    for index, row in test_queries_df.iterrows():\n",
    "        user_query = row['instruction_cleaned']\n",
    "        # The 'expected_response' is the original answer from the dataset.\n",
    "        # We can use this for a rough comparison, though the RAG system might phrase it differently.\n",
    "        expected_response = row['response_cleaned']\n",
    "\n",
    "        print(f\"\\n\\n--- Test Case: Query from Dataset ---\")\n",
    "        print(f\"Original Instruction (User Query): {user_query}\")\n",
    "        print(f\"Original Response (Expected): {expected_response[:250]}...\") # Show a snippet\n",
    "\n",
    "        _ = run_rag_query(\n",
    "            user_query,\n",
    "            embedding_model,\n",
    "            generative_model,\n",
    "            faiss_index,\n",
    "            kb_texts, # The list of text chunks from our knowledge base\n",
    "            kb_metadata, # Associated metadata\n",
    "            TOP_K_RETRIEVAL # How many chunks to retrieve\n",
    "        )\n",
    "\n",
    "    print(\"\\n--- RAG Pipeline Test Complete ---\")\n",
    "    print(\"\\nConsiderations for Improvement & Iteration:\")\n",
    "    print(\"1. Chunking Strategy: If responses are very long, split them into smaller, more focused chunks.\")\n",
    "    print(\"2. TOP_K_RETRIEVAL: Experiment with the number of chunks retrieved (k).\")\n",
    "    print(\"3. Prompt Engineering: Refine the prompt sent to Gemini for better control over output.\")\n",
    "    print(\"4. Evaluation: Systematically evaluate responses (e.g., against 'expected_response' or with human review).\")\n",
    "    print(\"5. Vector Database: For larger scale, consider Vertex AI Vector Search or other managed vector DBs.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping RAG pipeline execution as one or more necessary components (data, models, index) are missing.\")\n",
    "    print(\"Please ensure all previous cells have run successfully and all variables are defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5730fd26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RagGemini",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
